<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>分布式训练学习笔记</title>
    <link href="/2023/04/02/parrel-training/"/>
    <url>/2023/04/02/parrel-training/</url>
    
    <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%A6%82%E8%A6%81">并行训练概要</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C">数据并行</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C">模型并行</a></li></ul><!-- tocstop --><p>这篇博客记录了我在学习分布式训练的过程中的相关知识与实践经验。</p><h3 id="并行训练概要"><a href="#并行训练概要" class="headerlink" title="并行训练概要"></a>并行训练概要</h3><p>并行训练主要需要解决的痛点是两个：</p><ol><li>大模型训练时间过长</li><li>单个GPU显存有限，可能无法完整的容纳一个模型</li></ol><p>分布式训练技术可以通过将模型部署到多个GPU上来解决上述两个问题。可以分为数据并行、张量并行以及流水线并行</p><h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><h3 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读笔记《BaGuaLu:Targeting Brain Scale Pretrained Models with over 37 Million Cores》</title>
    <link href="/2023/04/02/paper-bagualu-1/"/>
    <url>/2023/04/02/paper-bagualu-1/</url>
    
    <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#%E8%83%8C%E6%99%AF">背景</a></li><li><a href="#methods">Methods</a><ul><li><a href="#%E8%8A%82%E7%82%B9%E5%86%85%E4%BC%98%E5%8C%96">节点内优化</a></li><li><a href="#%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5">混合并行策略</a></li><li><a href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83">混合精度训练</a></li></ul></li><li><a href="#%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0">性能评估</a></li><li><a href="#%E7%9B%B8%E5%85%B3%E9%98%85%E8%AF%BB%E9%93%BE%E6%8E%A5">相关阅读链接</a></li></ul><!-- tocstop --><p>这篇论文来自PPoPP‘22：<a href="https://pacman.cs.tsinghua.edu.cn/~zjd/publication/ppopp22-bagualu/ppopp22-bagualu.pdf">https://pacman.cs.tsinghua.edu.cn/~zjd/publication/ppopp22-bagualu/ppopp22-bagualu.pdf</a></p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>目前大模型因为使用了大量的参数使其精确度达到了领域前沿，比如CV、NLP等领域都有比较典型的大模型运用实例。这里作者举了MoE（Mixture of Experts）的例子，这是一种通过多种子专家网络和一个门控开关控制哪些子网络被选取的网络结构，这种网络设计已经在机器翻译等领域取得了成功。</p><p>但是目前大规模预训练模型仍然面临着一些问题，受到计算、存储、网络性能等多种因素的影响，因此目前的大模型是有潜力被继续拓展到更大规模的集群&#x2F;更多参数。</p><p>本文要部署大模型的平台是新一代神威超级计算机（New Generation Sunway Supercomputer，1 EFLOPS peek performance）,基于这个背景，作者提出了这项工作所面临的挑战，个人认为也是这篇文章的核心问题。</p><ol><li>硬件架构：超级计算机的架构大多是专门设计的，需要精心设计以使得软件应用于硬件架构有效结合 以充分发挥硬件的优势</li><li>存储容量：大模型和参数和中间结果需要占据大量的存储空间，为了放下这些内容我们需要对其进行划分，而不同的划分也会导致不同的通信策略</li><li>并行策略：要在超大规模的超算上部署就需要重新设计并行策略，比如上文的MoE在只使用数据并行时会带来大量的AllReduce开销</li><li>混合精度：传统科学计算问题多使用双精度，但是AI应用一般会使用单精度或者混合精度来最大化吞吐量。Sunway提供了多种精度，因为需要考虑混合精度如何充分利用的问题</li></ol><p>BaGuaLu 可以训练达到 14.5-trillion个参数使用1.002 EFLOPS的算力。BaGuaLu有能力拓展到174trillion个参数，这将和人脑中突触的数目相当。</p><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><h4 id="节点内优化"><a href="#节点内优化" class="headerlink" title="节点内优化"></a>节点内优化</h4><ol><li>核组调度：一个MPE负责计算，其他5个MPE负责处理通信相关以最大限度降低通信开销，减少进程数<br><img src="/2023/04/02/paper-bagualu-1/1.png" alt="1"></li><li>内存划分：每一行都由位于对角线上的CPE接受DMA请求，之后再通过RMA传输<br><img src="/2023/04/02/paper-bagualu-1/2.png" alt="2"></li></ol><h4 id="混合并行策略"><a href="#混合并行策略" class="headerlink" title="混合并行策略"></a>混合并行策略</h4><ol><li><p>Hybrid MoE parallelism and data parallelism strategy(MoDa)：模型并行与数据并行混合策略。也是并行训练中经常用到的优化方法。具体到本文中主要指针对神威的节点互联架构。如下图所示，对于右边这种胖树结构的节点布置，supernode间的通讯带宽是supernode内的通信带宽3&#x2F;16，因此需要考虑如何安排AlltoAll和AllReduce。文章中提到，经过测试发现AllReduce的次数是AlltoAll的16-24倍，因此将AllReduce安排在supernode内做，以减少通讯时间。<br><img src="/2023/04/02/paper-bagualu-1/Sunway.png" alt="3"></p></li><li><p>Parallel partition-based optimizer (ParO)：分割存储优化器等策略。这种优化策略在DeepSpeed提出的ZeRO中已经有过实现，主要是将梯度、参数、优化器等信息分别存储在不同的进程中以压缩存储空间。由于模型的规模较大，因此在本文中optimizer占据的空间可能只到其他数据的5%左右（参数、梯度、中间结果等）。因此，ParO只将十分之一到七分之一的参数进行全局共享。模型中的worker按照2D-grid存储，梯度首先在supernode内进行Reduce-Scatter（一个维度）之后在supernode间进行All-Reduce（另一个维度），更新之后，参数再在supernode内进行All-Gather。这种分割存储optimizer的方式有另一个好处，就是可以很方便的存储模型的checkpoint。</p></li><li><p>SunWay Imbalance Proficiently Eliminated (SWIPE)：这是一种实现负载均衡的策略。因为MoE存在的固有问题就是每次迭代都会有一部分的Expert处理很多的输入，而有一些Expert又很空闲。因此本文设计了一种算法：每个worker持有一个expert，接受一份输入，每个expert最后得到的样本数要求是相同的。首先获得每个输入样本对各个expert的打分，让每个样本去打分最高的expert那里，经过这一步之后，就会有一些样本溢出的和部分空闲的expert，之后每个worker根据对这个溢出数量和空闲容量表的查询进行分发，以达到负载均衡的目的。这样的算法只需要常数次通讯次数。<br><img src="/2023/04/02/paper-bagualu-1/3.png" alt="4"></p></li></ol><h4 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h4><p>这里首先提出了常规大模型训练使用的一些混合精度策略，如NVIDIA的APEX：</p><ul><li>O0: FP32 training.</li><li>O1: Use FP16 in certain operators, such as GEMM or convolution. The inputs will be cast before and after the mixedprecision operation.</li><li>O2: Cast weights and input data to FP16 and maintain an FP32 master weight for optimization.</li><li>O3: FP16 training.</li></ul><p>在NVIDIA GPU上一般O1、O2的策略都能取得与O0相同的效果，但是在神威上这个结论却不生效。O1策略由于存在大量的数据类型转换，而神威的内存带宽较低，从而严重影响性能。而O2是因为FP16的数据过多造成了大量的overflow出现使得模型难以收敛。</p><p>因此本文工作就对模型的每一层都进行不同策略实验，得到了最优配置如下。<br><img src="/2023/04/02/paper-bagualu-1/4.png" alt="5"></p><h3 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h3><p>略</p><h3 id="相关阅读链接"><a href="#相关阅读链接" class="headerlink" title="相关阅读链接"></a>相关阅读链接</h3><ol><li><a href="https://laekov.com.cn/view/181401#howto">https://laekov.com.cn/view/181401#howto</a></li><li><a href="https://github.com/laekov/fastmoe">https://github.com/laekov/fastmoe</a></li><li><a href="https://youtu.be/T3-6WH1GyRw">https://youtu.be/T3-6WH1GyRw</a></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
