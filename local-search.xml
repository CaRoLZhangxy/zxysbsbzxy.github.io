<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>致以逝去的朋友——《2023年1月表深渊闪灵同调卡组攻略文档》</title>
    <link href="/2023/04/07/ygo-note-1/"/>
    <url>/2023/04/07/ygo-note-1/</url>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本文要讲的卡组是以深渊兽、卫星闪灵以及一定数量的光暗2星调整组成，通过资源调度做出混沌魔龙积累大量墓地资源，最终目的是做出淘气精灵链接端的两只同调怪兽以及卫星闪灵系统的软阻抗。由于游戏王四月表的更新，深渊之兽的核心赫界龙和卫星闪灵的核心蓝色喷流灵都遭到了限一规制，因此本套卡组也就不复存在。这篇文章的意义也不过是纪念我的第一次参加大型比赛时所用的卡组，当然也在这套卡组上投入了不少心血。</p><h2 id="卡组构筑"><a href="#卡组构筑" class="headerlink" title="卡组构筑"></a>卡组构筑</h2><p><img src="/2023/04/07/ygo-note-1/%E4%B8%BB%E5%8D%A1%E7%BB%84.png"><br><img src="/2023/04/07/ygo-note-1/extra-side.png"></p><p>深渊兽方面：由于玛格巨龙在1月表限一，因此对能提供回合内精堆能力的萨隆魔龙下三，这也是为了保证卡组内深渊兽的浓度，以针对环境的定义者——珠泪卡组，这也是本卡组在1月表环境能够生存的一点原因。</p><p>卫星闪灵方面：采取常规闪灵小轴数量，除小蓝外其他全部只下1.</p><p>调整方面：尸忍1，百檎龙2，强袭同调士2。由于卡组内卫星龙炮的存在，能够提供精堆两星调整的能力，因此对于调整的浓度要求不高。同时调整上手大多需求通招（特招的强袭同调士有同调自肃），因此在最终卡组调整的时候减少了卡组内2星调整的浓度。</p><p>手坑：常规3灰3g3指，多携带了一张幽鬼兔用于针对环境内的超重系卡组，同时对俱舍纯爱等卡组也有一定打击，同时自身也是光属性，在墓地能够给深渊兽提供起跳的对象，也可以被混沌魔龙检索上手，实战中效果不错。由于本卡组携带了大量深渊兽，因此在后手对局中也能提供给对手大量回合外干扰。三战之才，高power卡，吃到手坑时的反制手段。</p><p>其他：电子界小工具，本卡组最强通招点，一卡卫星龙炮，详细使用方法会在下面单卡介绍中记述。混沌领域，赫界龙检索点，进墓的抽卡回收补点。乌爪鸦，小爱神可以检索的两星补点，不如连锁特招，相比缓存猫娃使用体验更好一点。</p><h2 id="单卡分析"><a href="#单卡分析" class="headerlink" title="单卡分析"></a>单卡分析</h2><p><img src="/2023/04/07/ygo-note-1/%E8%B5%AB%E7%95%8C%E9%BE%99.png"><br>深渊之兽赫界龙，深渊兽系统的调度核心，用法很多。</p><p>首先是检索，检索首先考虑手牌中深渊之兽的种类，优先检索手牌不存在的深渊兽，其次考虑墓地光暗资源，如果墓地只有赫界龙一张光暗，则可以检索萨隆魔龙，萨隆魔龙除外赫界龙起跳，当其送墓时可以再堆一张赫界龙保证墓地中有赫界龙可以起跳。当墓地有其他光暗起跳点，手牌资源一般时可检索玛格巨龙，在ep检索其他深渊兽提供下回合的干扰能力。最后如果手牌资源很充足，也可以检索萨隆堆墓绿阵，ep红阵把绿阵贴上来，提供更立体的终场，当然这种展开也丧失了当回合回一抽一的赚资源点。</p><p>赫界龙的特招一般依靠吃掉场上存在的深渊兽和混沌魔龙。2效果贴永续的选择大多数情况下选择绿阵赚资源，也可以选择红阵提供后场除去解场能力，具体使用比较灵活。（一般看哪个贴纸还没被混沌魔龙堆进墓地（x））</p><p><img src="/2023/04/07/ygo-note-1/%E9%B8%9F%E9%BE%99.png"><br>鸟龙的主要功能是被玛格巨龙检索提供下回合一次除外的能力，理想情况下是解放当回合绿阵拉出来的发过效果的玛格巨龙发动效果，也可以与德鲁伊鳞虫联动触发其送墓效果。通常不是赫界龙的优先检索卡。<br><img src="/2023/04/07/ygo-note-1/%E5%BE%B7%E9%B2%81%E4%BC%8A.png"><br>德鲁伊鳞虫的主要作用是提供一次除去的能力，回合外干扰和回合内解场的能力都很强大。其送墓解场的效果也能解决环境内的No.41等卡片。<br><img src="/2023/04/07/ygo-note-1/%E7%8E%9B%E6%A0%BC%E5%B7%A8%E9%BE%99.png"><br>玛格的作用主要是提供一次广范围的龙族检索，可以当回合发效果，下回合被绿阵拉出来再发效果，提供很强大的续航能力，其单卡power很高。但该卡的尴尬之处也在于其检索是ep处理，对于当回合的展开没有帮助，同时实战中很难遇到玛格发挥作用的拉锯战，所以是张蛮尴尬的卡，不过由于其赚卡能力，依然是赫界龙的优先检索点。<br><img src="/2023/04/07/ygo-note-1/%E8%90%A8%E9%9A%86.png"><br>萨隆，深渊兽的大腿，当回合堆墓的能力，能够堆下赫界龙以及绿阵。其作为混沌魔龙的同调素材进墓时，也能够自排连锁帮助混沌魔龙躲灰。</p><p><img src="/2023/04/07/ygo-note-1/%E5%B0%8F%E5%B7%A5%E5%85%B7.png"><br>本卡组最强通招点，一卡卫星龙炮。具体使用方式是：通招电子界工具，如果先手的话墓地一般没有两星可以拉，后手可以拉先手丢出的增殖的G（如果有的话），当然也可以模仿废二玩家在自己回合丢G，然后用电子界工具拉起来，通常没有必要，因为大多数情况并不缺这一个两星补点。电子界工具链接召唤link1连环栗子球，进墓发动效果生一个两星token。这一步有两个关键，一个是连环栗子球能够提供自己回合的一次陷阱康，帮助混沌魔龙躲避无限泡影或者后手应对对手红坑的对策；另一个是token的存在能够提供给百檎龙1效果场上存在效果怪兽以外的怪兽可以特招的能力，场上的2星token加上百檎龙即可达成同调召唤混沌魔龙的目的（出小爱神加百檎龙墓地效果）。如果上一步还没有获得能够召唤混沌魔龙的场面，那么下一步可以链接召唤link2卫星龙炮堆墓尸忍，手牌的深渊兽除外尸忍起跳，即可同调召唤混沌魔龙。</p><p>另一个本人在实战中经常使用的技巧是后手，如果场上有上回合特招过的深渊兽，通招电子界工具可以同调召唤混沌之双翼，给自己的同调怪兽提供伪全抗的抗性加上一次取对象的除外能力，这也依赖于对方觉得你场上没有两星没有调整而不交阻抗的大意，但一旦完成power还是很高的。</p><p>其实电子界工具的厉害之处在于它表面上与本卡组的“格格不入”的特征，当对手看到你通招电子界工具时，通常会因为不知道你下一步会怎么做陷入犹豫，而下一步的动作则往往会逼出对手不可承受的资源。其送墓生token的能力也不怕常规意义上的除去手段，本质上是一种信息差战略。</p><p>（施工中）</p>]]></content>
    
    
    
    <tags>
      
      <tag>游戏王ocg</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式训练学习笔记</title>
    <link href="/2023/04/02/parrel-training/"/>
    <url>/2023/04/02/parrel-training/</url>
    
    <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%A6%82%E8%A6%81">并行训练概要</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C">数据并行</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C">模型并行</a></li></ul><!-- tocstop --><p>这篇博客记录了我在学习分布式训练的过程中的相关知识与实践经验。</p><h3 id="并行训练概要"><a href="#并行训练概要" class="headerlink" title="并行训练概要"></a>并行训练概要</h3><p>并行训练主要需要解决的痛点是两个：</p><ol><li>大模型训练时间过长</li><li>单个GPU显存有限，可能无法完整的容纳一个模型</li></ol><p>分布式训练技术可以通过将模型部署到多个GPU上来解决上述两个问题。可以分为数据并行、张量并行以及流水线并行</p><h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><h3 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h3>]]></content>
    
    
    
    <tags>
      
      <tag>学习笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文阅读笔记《BaGuaLu:Targeting Brain Scale Pretrained Models with over 37 Million Cores》</title>
    <link href="/2023/04/02/paper-bagualu-1/"/>
    <url>/2023/04/02/paper-bagualu-1/</url>
    
    <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#%E8%83%8C%E6%99%AF">背景</a></li><li><a href="#methods">Methods</a><ul><li><a href="#%E8%8A%82%E7%82%B9%E5%86%85%E4%BC%98%E5%8C%96">节点内优化</a></li><li><a href="#%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5">混合并行策略</a></li><li><a href="#%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83">混合精度训练</a></li></ul></li><li><a href="#%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0">性能评估</a></li><li><a href="#%E7%9B%B8%E5%85%B3%E9%98%85%E8%AF%BB%E9%93%BE%E6%8E%A5">相关阅读链接</a></li></ul><!-- tocstop --><p>这篇论文来自PPoPP‘22：<a href="https://pacman.cs.tsinghua.edu.cn/~zjd/publication/ppopp22-bagualu/ppopp22-bagualu.pdf">https://pacman.cs.tsinghua.edu.cn/~zjd/publication/ppopp22-bagualu/ppopp22-bagualu.pdf</a></p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>目前大模型因为使用了大量的参数使其精确度达到了领域前沿，比如CV、NLP等领域都有比较典型的大模型运用实例。这里作者举了MoE（Mixture of Experts）的例子，这是一种通过多种子专家网络和一个门控开关控制哪些子网络被选取的网络结构，这种网络设计已经在机器翻译等领域取得了成功。</p><p>但是目前大规模预训练模型仍然面临着一些问题，受到计算、存储、网络性能等多种因素的影响，因此目前的大模型是有潜力被继续拓展到更大规模的集群&#x2F;更多参数。</p><p>本文要部署大模型的平台是新一代神威超级计算机（New Generation Sunway Supercomputer，1 EFLOPS peek performance）,基于这个背景，作者提出了这项工作所面临的挑战，个人认为也是这篇文章的核心问题。</p><ol><li>硬件架构：超级计算机的架构大多是专门设计的，需要精心设计以使得软件应用于硬件架构有效结合 以充分发挥硬件的优势</li><li>存储容量：大模型和参数和中间结果需要占据大量的存储空间，为了放下这些内容我们需要对其进行划分，而不同的划分也会导致不同的通信策略</li><li>并行策略：要在超大规模的超算上部署就需要重新设计并行策略，比如上文的MoE在只使用数据并行时会带来大量的AllReduce开销</li><li>混合精度：传统科学计算问题多使用双精度，但是AI应用一般会使用单精度或者混合精度来最大化吞吐量。Sunway提供了多种精度，因为需要考虑混合精度如何充分利用的问题</li></ol><p>BaGuaLu 可以训练达到 14.5-trillion个参数使用1.002 EFLOPS的算力。BaGuaLu有能力拓展到174trillion个参数，这将和人脑中突触的数目相当。</p><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><h4 id="节点内优化"><a href="#节点内优化" class="headerlink" title="节点内优化"></a>节点内优化</h4><ol><li>核组调度：一个MPE负责计算，其他5个MPE负责处理通信相关以最大限度降低通信开销，减少进程数<br><img src="/2023/04/02/paper-bagualu-1/1.png" alt="1"></li><li>内存划分：每一行都由位于对角线上的CPE接受DMA请求，之后再通过RMA传输<br><img src="/2023/04/02/paper-bagualu-1/2.png" alt="2"></li></ol><h4 id="混合并行策略"><a href="#混合并行策略" class="headerlink" title="混合并行策略"></a>混合并行策略</h4><ol><li><p>Hybrid MoE parallelism and data parallelism strategy(MoDa)：模型并行与数据并行混合策略。也是并行训练中经常用到的优化方法。具体到本文中主要指针对神威的节点互联架构。如下图所示，对于右边这种胖树结构的节点布置，supernode间的通讯带宽是supernode内的通信带宽3&#x2F;16，因此需要考虑如何安排AlltoAll和AllReduce。文章中提到，经过测试发现AllReduce的次数是AlltoAll的16-24倍，因此将AllReduce安排在supernode内做，以减少通讯时间。<br><img src="/2023/04/02/paper-bagualu-1/Sunway.png" alt="3"></p></li><li><p>Parallel partition-based optimizer (ParO)：分割存储优化器等策略。这种优化策略在DeepSpeed提出的ZeRO中已经有过实现，主要是将梯度、参数、优化器等信息分别存储在不同的进程中以压缩存储空间。由于模型的规模较大，因此在本文中optimizer占据的空间可能只到其他数据的5%左右（参数、梯度、中间结果等）。因此，ParO只将十分之一到七分之一的参数进行全局共享。模型中的worker按照2D-grid存储，梯度首先在supernode内进行Reduce-Scatter（一个维度）之后在supernode间进行All-Reduce（另一个维度），更新之后，参数再在supernode内进行All-Gather。这种分割存储optimizer的方式有另一个好处，就是可以很方便的存储模型的checkpoint。</p></li><li><p>SunWay Imbalance Proficiently Eliminated (SWIPE)：这是一种实现负载均衡的策略。因为MoE存在的固有问题就是每次迭代都会有一部分的Expert处理很多的输入，而有一些Expert又很空闲。因此本文设计了一种算法：每个worker持有一个expert，接受一份输入，每个expert最后得到的样本数要求是相同的。首先获得每个输入样本对各个expert的打分，让每个样本去打分最高的expert那里，经过这一步之后，就会有一些样本溢出的和部分空闲的expert，之后每个worker根据对这个溢出数量和空闲容量表的查询进行分发，以达到负载均衡的目的。这样的算法只需要常数次通讯次数。一个更近一步的优化在与本文相关的另一篇PPoPP’22的文章中阐述（<a href="https://dl.acm.org/doi/10.1145/3503221.3508418%EF%BC%89%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%AE%9E%E7%8E%B0%E4%B8%80%E7%A7%8D%E8%80%83%E8%99%91%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91%E7%9A%84%E9%97%A8%E7%BD%91%E7%BB%9C%EF%BC%88Topology-aware">https://dl.acm.org/doi/10.1145/3503221.3508418），可以实现一种考虑网络拓扑的门网络（Topology-aware</a> Gate），考虑硬件连接的拓扑关系. 限制跨机器选择专家的输入数量.让多余的输入按照分数重新在本节点内选择一个喜欢的专家来进行处理，即考虑了节点间通讯延迟的gate。<br><img src="/2023/04/02/paper-bagualu-1/3.png" alt="4"></p></li></ol><h4 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h4><p>这里首先提出了常规大模型训练使用的一些混合精度策略，如NVIDIA的APEX：</p><ul><li>O0: FP32 training.</li><li>O1: Use FP16 in certain operators, such as GEMM or convolution. The inputs will be cast before and after the mixedprecision operation.</li><li>O2: Cast weights and input data to FP16 and maintain an FP32 master weight for optimization.</li><li>O3: FP16 training.</li></ul><p>在NVIDIA GPU上一般O1、O2的策略都能取得与O0相同的效果，但是在神威上这个结论却不生效。O1策略由于存在大量的数据类型转换，而神威的内存带宽较低，从而严重影响性能。而O2是因为FP16的数据过多造成了大量的overflow出现使得模型难以收敛。</p><p>因此本文工作就对模型的每一层都进行不同策略实验，得到了最优配置如下。<br><img src="/2023/04/02/paper-bagualu-1/4.png" alt="5"></p><h3 id="性能评估"><a href="#性能评估" class="headerlink" title="性能评估"></a>性能评估</h3><p>略</p><h3 id="相关阅读链接"><a href="#相关阅读链接" class="headerlink" title="相关阅读链接"></a>相关阅读链接</h3><ol><li><a href="https://laekov.com.cn/view/181401#howto">https://laekov.com.cn/view/181401#howto</a></li><li><a href="https://github.com/laekov/fastmoe">https://github.com/laekov/fastmoe</a></li><li><a href="https://youtu.be/T3-6WH1GyRw">https://youtu.be/T3-6WH1GyRw</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
