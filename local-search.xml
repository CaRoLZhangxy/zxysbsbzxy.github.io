<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>分布式训练学习笔记</title>
    <link href="/2023/03/21/parallel-traing/"/>
    <url>/2023/03/21/parallel-traing/</url>
    
    <content type="html"><![CDATA[<!-- toc --><ul><li><a href="#%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%A6%82%E8%A6%81">并行训练概要</a></li><li><a href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C">数据并行</a></li><li><a href="#%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C">模型并行</a></li></ul><!-- tocstop --><p>这篇博客记录了我在学习分布式训练的过程中的相关知识与实践经验。</p><h3 id="并行训练概要"><a href="#并行训练概要" class="headerlink" title="并行训练概要"></a>并行训练概要</h3><p>并行训练主要需要解决的痛点是两个：</p><ol><li>大模型训练时间过长</li><li>单个GPU显存有限，可能无法完整的容纳一个模型</li></ol><p>分布式训练技术可以通过将模型部署到多个GPU上来解决上述两个问题。可以分为数据并行、张量并行以及流水线并行</p><h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><h3 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h3>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
